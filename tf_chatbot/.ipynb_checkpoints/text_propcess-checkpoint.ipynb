{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii\n",
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11263403298992318512\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import sys\n",
    "print sys.getdefaultencoding()\n",
    "\n",
    "import codecs\n",
    "import data_utils\n",
    "import seq2seq_model\n",
    "import re\n",
    "\n",
    "import jieba\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "\n",
    "# chinese comma \n",
    "# numbers \n",
    "# @ symbol \n",
    "\n",
    "# TO DO:\n",
    "# 1. merge vocabulary library \n",
    "# 2. beam search on decoder\n",
    "# 3. sampled softmax\n",
    "\n",
    "\n",
    "# https://blog.kovalevskyi.com/rnn-based-chatbot-for-6-hours-b847d2d92c43\n",
    "# http://www.franksworld.com/2017/04/05/how-to-make-a-deep-learning-chatbot-with-keras-and-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions for data splitting \n",
    "\n",
    "def remove_symbol_at( sentence ):\n",
    "    withinAt=False\n",
    "    res = []\n",
    "    for i in sentence:\n",
    "        if i=='@':\n",
    "            if withinAt == True:\n",
    "                withinAt = False\n",
    "            else:\n",
    "                withinAt = True\n",
    "            \n",
    "        elif withinAt == False:\n",
    "            res.append(i)\n",
    "            withinAt = False\n",
    "            \n",
    "    return res\n",
    "\n",
    "def remove_number(sentence):\n",
    "    _DIGIT_RE = re.compile(br\"\\d\")\n",
    "    res = []\n",
    "    \n",
    "    for i in sentence:\n",
    "        if re.sub(_DIGIT_RE, b\"0\", i) == i:\n",
    "            res.append(i)\n",
    "    return res \n",
    "\n",
    "def normalize_number(sentence):\n",
    "    _DIGIT_RE = re.compile(br\"\\d\")\n",
    "    res = []\n",
    "    \n",
    "    for i in sentence:\n",
    "        res.append(re.sub(_DIGIT_RE, b\"0\", i))\n",
    "    return res \n",
    "\n",
    "def print_out_cn( sentence ):\n",
    "    for i in sentence:\n",
    "        print i.encode('utf-8')\n",
    "\n",
    "def file_encoder_decoder_pair( enc_path, dec_path, text_line):\n",
    "    \n",
    "    tmp = text_line.split('\\t')\n",
    "    \n",
    "    if len(tmp)<2:\n",
    "        return False\n",
    "    \n",
    "    enc_str = remove_symbol_at( tmp[0].strip() )\n",
    "    dec_str = remove_symbol_at( tmp[1].strip() )\n",
    "    \n",
    "    enc_str = remove_number( enc_str )\n",
    "    dec_str = remove_number( dec_str )\n",
    "    \n",
    "    with open(enc_path, \"a\") as enc_file:\n",
    "        \n",
    "        for j in enc_str:\n",
    "            enc_file.write(\"%s\" %j.encode('utf-8'))\n",
    "            \n",
    "        enc_file.write(\"\\n\")\n",
    "        \n",
    "    with open(dec_path, \"a\") as dec_file:\n",
    "        \n",
    "        for j in dec_str:\n",
    "            dec_file.write(\"%s\" %j.encode('utf-8'))\n",
    "            \n",
    "        dec_file.write(\"\\n\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def file_ini( path ):\n",
    "    with open(path, \"w\") as file_handler:\n",
    "        file_handler.close()\n",
    "        \n",
    "# TO DO: \n",
    "def tokenize_symbol_at( sentence ):\n",
    "    return 0       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# read and split the dataset \n",
    "import codecs\n",
    "\n",
    "# source data format:\n",
    "# a set of pairs of < question \\t answer > \n",
    "# \n",
    "\n",
    "src_file_path = '../qa_fanbingbing.txt'\n",
    "\n",
    "f1 = codecs.open(src_file_path, \"r\", \"utf-8\")\n",
    "\n",
    "num_lines = sum(1 for line in open(src_file_path))\n",
    "num_train = int(0.8*num_lines)\n",
    "\n",
    "file_ini( \"data_cn/train.cn.enc\" )\n",
    "file_ini( \"data_cn/train.cn.dec\" )\n",
    "file_ini( \"data_cn/test.cn.enc\" )\n",
    "file_ini( \"data_cn/test.cn.dec\" )\n",
    "\n",
    "# training data\n",
    "for i in range(num_train):\n",
    "    text = f1.readline()\n",
    "    if not file_encoder_decoder_pair( 'data_cn/train.cn.enc', 'data_cn/train.cn.dec', text):\n",
    "        print i, text\n",
    "\n",
    "# testing data\n",
    "cnt = i\n",
    "text = f1.readline()\n",
    "while text:\n",
    "    cnt+=1\n",
    "    if not file_encoder_decoder_pair( 'data_cn/test.cn.enc',  'data_cn/test.cn.dec', text):\n",
    "        print cnt, text\n",
    "    text = f1.readline()\n",
    "\n",
    "print 'DONE!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_cn/train.cn.enc.ids20000    Mean: 24.8435897436 Max: 107 Min: 1\n",
      "data_cn/train.cn.dec.ids20000    Mean: 42.8653846154 Max: 103 Min: 1\n",
      "data_cn/test.cn.enc.ids20000    Mean: 22.0820512821 Max: 81 Min: 2\n",
      "data_cn/test.cn.dec.ids20000    Mean: 45.9846153846 Max: 107 Min: 1\n"
     ]
    }
   ],
   "source": [
    "# statistics on encoder-decoder pairs\n",
    "\n",
    "file_paths = [ \"data_cn/train.cn.enc.ids20000\", \"data_cn/train.cn.dec.ids20000\",\\\n",
    "               \"data_cn/test.cn.enc.ids20000\",  \"data_cn/test.cn.dec.ids20000\" ]\n",
    "\n",
    "for i in file_paths:\n",
    "    \n",
    "    f1 = codecs.open(i, \"r\")\n",
    "        \n",
    "    line = f1.readline()\n",
    "    line_len = []\n",
    "    \n",
    "    while line:\n",
    "        line_len.append( len(line.split(' ')) )\n",
    "        line = f1.readline()\n",
    "        \n",
    "    print i,\"  \", \\\n",
    "          \"Mean:\", sum(line_len)*1.0/len(line_len),\\\n",
    "          \"Max:\", max(line_len),\\\n",
    "          \"Min:\", min(line_len)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不畏\n",
      "险阻\n",
      "，\n",
      "不\n",
      "忘\n",
      "初心\n",
      ":\n",
      "每\n",
      "一\n",
      "段\n",
      "故事\n",
      "，\n",
      "每\n",
      "一\n",
      "句\n",
      "话\n",
      "，\n",
      "都\n",
      "和\n",
      "前进\n",
      "路\n",
      "上\n",
      "的\n",
      "你\n",
      "一起\n",
      "共勉\n",
      "。\n",
      "冰冰\n",
      "聊\n",
      "得\n",
      "很\n",
      "尽兴\n",
      "，\n",
      "没\n",
      "看\n",
      "节目\n",
      "的\n",
      "小伙伴\n",
      "快\n",
      "戳\n",
      "➡️➡️\n"
     ]
    }
   ],
   "source": [
    "# test on snownlp and jieba\n",
    "from snownlp import *\n",
    "\n",
    "sentence = u\"   从过去获取记忆的灵感，创造121明天未知12123的潮流，212。@范冰冰@@范冰冰工作室@ 【范冰冰 】@路易威登@： ...\"\n",
    "\n",
    "sentence = u\"海报来也！大家一起数星星→       梁家辉 祖峰    众神归位！请用一个词点评本片阵容，\\\n",
    "评论里最有才的那位，片方将免费送你去上影节“全明星封神之夜”哟！\"\n",
    "\n",
    "sentence = u'???? ????<br/>奥汀子集齐，月刊召唤成功！ 、 、 、 、 、 、 及 携手呈现双封面！明天起将每天揭晓位主创的网络专属单人封面，\\\n",
    "更有可能获得独属你家偶像的签名海报套装，敬请锁定陆续开启的购买链接 ...'\n",
    "\n",
    "sentence = u'期待值up up but fdafa！ 大哥英雄力MAX 星约翰尼•诺克斯维尔,在澳大利亚总理夫人Lucy Turnbull女士的见证下\\\n",
    "先转藏再看by网络 http:t.cn/Rq0Erd'\n",
    "\n",
    "sentence = u'冰冰身着马海毛121运动双排扣大衣132323'\n",
    "\n",
    "sentence = u'不畏险阻，不忘初心 :每一段故事，每一句话，都和前进路上的你一起共勉。冰冰聊得很尽兴，没看节目的小伙伴快戳➡️➡️'\n",
    "# snownlp\n",
    "s = SnowNLP(sentence)\n",
    "_DIGIT_RE = re.compile(br\"\\d\")\n",
    "\n",
    "# split on words\n",
    "for i in s.words:\n",
    "    \n",
    "    word = re.sub(_DIGIT_RE, b\"0\", i)\n",
    "    if word == i:\n",
    "        print word.encode('utf-8')\n",
    "    else:\n",
    "        print word.encode('utf-8')\n",
    "        \n",
    "        \n",
    "# # jieba\n",
    "# seg_list = jieba.cut(sentence)\n",
    "\n",
    "# tokens = []\n",
    "# for i in seg_list:\n",
    "#     print i\n",
    "#     tokens.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'dec_vocab_size': 20000,\n",
       " 'enc_vocab_size': 20000,\n",
       " 'layer_size': 256,\n",
       " 'learning_rate': 0.5,\n",
       " 'learning_rate_decay_factor': 0.99,\n",
       " 'max_gradient_norm': 5.0,\n",
       " 'max_train_data_size': 0,\n",
       " 'mode': 'test',\n",
       " 'num_layers': 3,\n",
       " 'steps_per_checkpoint': 500,\n",
       " 'test_dec': 'data_cn/test.cn.dec',\n",
       " 'test_enc': 'data_cn/test.cn.enc',\n",
       " 'train_dec': 'data_cn/train.cn.dec',\n",
       " 'train_enc': 'data_cn/train.cn.enc',\n",
       " 'working_directory': 'working_dir_cn/'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    from ConfigParser import SafeConfigParser\n",
    "except:\n",
    "    from configparser import SafeConfigParser\n",
    "    \n",
    "def get_config(config_file='seq2seq.ini'):\n",
    "    parser = SafeConfigParser()\n",
    "    parser.read(config_file)\n",
    "    # get the ints, floats and strings\n",
    "    _conf_ints = [ (key, int(value)) for key,value in parser.items('ints') ]\n",
    "    _conf_floats = [ (key, float(value)) for key,value in parser.items('floats') ]\n",
    "    _conf_strings = [ (key, str(value)) for key,value in parser.items('strings') ]\n",
    "    return dict(_conf_ints + _conf_floats + _conf_strings)\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "\n",
    "gConfig = get_config()\n",
    "\n",
    "gConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "\n",
      "Use existing vocabulary\n",
      "Use existing vocabulary\n",
      "Tokenizing data in data_cn/train.cn.enc\n",
      "Tokenizing data in data_cn/train.cn.dec\n",
      "Tokenizing data in data_cn/test.cn.enc\n",
      "Tokenizing data in data_cn/test.cn.dec\n"
     ]
    }
   ],
   "source": [
    "from data_utils import *\n",
    "\n",
    "enc_train, dec_train, enc_dev, dec_dev, _, _ = \\\n",
    "data_utils.prepare_custom_data(gConfig['working_directory'],gConfig['train_enc'],gConfig['train_dec'],\\\n",
    "                    gConfig['test_enc'],gConfig['test_dec'],gConfig['enc_vocab_size'],gConfig['dec_vocab_size'] )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
