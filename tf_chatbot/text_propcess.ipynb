{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascii\n",
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2811332191905368984\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import sys\n",
    "print sys.getdefaultencoding()\n",
    "\n",
    "import codecs\n",
    "import data_utils\n",
    "import seq2seq_model\n",
    "import re\n",
    "\n",
    "import jieba\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "\n",
    "# chinese comma \n",
    "# numbers \n",
    "# @ symbol \n",
    "\n",
    "\n",
    "# TO DO:\n",
    "# 1. merge or agumented vocabulary library \n",
    "# 2. beam search on decoder, diversified beam search\n",
    "# 3. sampled softmax\n",
    "# 4. word embedding \n",
    "# 5. phrase based decoder \n",
    "# 6. intention\n",
    "# 7. style transfer: speaker style, \n",
    "\n",
    "\n",
    "# https://blog.kovalevskyi.com/rnn-based-chatbot-for-6-hours-b847d2d92c43\n",
    "# http://www.franksworld.com/2017/04/05/how-to-make-a-deep-learning-chatbot-with-keras-and-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  reference: https://github.com/candlewill/Dialog_Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions for data splitting \n",
    "\n",
    "def remove_symbol_at( sentence ):\n",
    "    withinAt=False\n",
    "    res = []\n",
    "    for i in sentence:\n",
    "        if i=='@':\n",
    "            if withinAt == True:\n",
    "                withinAt = False\n",
    "            else:\n",
    "                withinAt = True\n",
    "            \n",
    "        elif withinAt == False:\n",
    "            res.append(i)\n",
    "            withinAt = False\n",
    "            \n",
    "    return res\n",
    "\n",
    "def remove_number(sentence):\n",
    "    _DIGIT_RE = re.compile(br\"\\d\")\n",
    "    res = []\n",
    "    \n",
    "    for i in sentence:\n",
    "        if re.sub(_DIGIT_RE, b\"0\", i) == i:\n",
    "            res.append(i)\n",
    "    return res \n",
    "\n",
    "def normalize_number(sentence):\n",
    "    _DIGIT_RE = re.compile(br\"\\d\")\n",
    "    res = []\n",
    "    \n",
    "    for i in sentence:\n",
    "        res.append(re.sub(_DIGIT_RE, b\"0\", i))\n",
    "    return res \n",
    "\n",
    "def print_out_cn( sentence ):\n",
    "    for i in sentence:\n",
    "        print i.encode('utf-8')\n",
    "\n",
    "def file_encoder_decoder_pair( enc_path, dec_path, text_line, idx):\n",
    "    \n",
    "    tmp = text_line.split('\\t')\n",
    "#     print len(tmp)\n",
    "    \n",
    "    if len(tmp)<2:\n",
    "        return False\n",
    "    \n",
    "    enc_str = remove_symbol_at( tmp[idx].strip() )\n",
    "    dec_str = remove_symbol_at( tmp[idx+1].strip() )\n",
    "    \n",
    "    enc_str = remove_number( enc_str )\n",
    "    dec_str = remove_number( dec_str )\n",
    "    \n",
    "    with open(enc_path, \"a\") as enc_file:\n",
    "        \n",
    "        for j in enc_str:\n",
    "            enc_file.write(\"%s\" %j.encode('utf-8'))\n",
    "            \n",
    "        enc_file.write(\"\\n\")\n",
    "        \n",
    "    with open(dec_path, \"a\") as dec_file:\n",
    "        \n",
    "        for j in dec_str:\n",
    "            dec_file.write(\"%s\" %j.encode('utf-8'))\n",
    "            \n",
    "        dec_file.write(\"\\n\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def file_ini( path ):\n",
    "    with open(path, \"w\") as file_handler:\n",
    "        file_handler.close()\n",
    "        \n",
    "# TO DO: \n",
    "def tokenize_symbol_at( sentence ):\n",
    "    return 0       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113 来大济南啦@！映后见面会主创们和观众聊的很开心，冰冰还用家乡话跟朋友们问好！下一站沈阳，明天见妆发：@张浩然Rstudio@服装：@ChristopherBu@\n",
      "\n",
      "4429  ",
      "\n",
      "4430  ",
      "\n",
      "4431 但得而不待，时不再来生命无关途中际遇，兀自向前 ",
      "\n",
      "4432  ",
      "\n",
      "4433 终于明白，幸福不在终点，幸福就是此生此路 ",
      "\n",
      "4434 请珍视每一寸光阴，珍视每一个共同渡过的人相信同伴就是一种珍贵AlfredDSouza\n",
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# read and split the dataset \n",
    "import codecs\n",
    "\n",
    "# source data format:\n",
    "# a set of pairs of < question \\t answer > \n",
    "# \n",
    "\n",
    "src_file_path = '../../dataset/nlp/huarui/huarui.weibo.all.star.QA.txt'\n",
    "\n",
    "f1 = codecs.open(src_file_path, \"r\", \"utf-8\")\n",
    "\n",
    "num_lines = sum(1 for line in open(src_file_path))\n",
    "num_train = int(0.8*num_lines)\n",
    "\n",
    "file_ini( \"../../dataset/nlp/data_cn/train.cn.enc\" )\n",
    "file_ini( \"../../dataset/nlp/data_cn/train.cn.dec\" )\n",
    "file_ini( \"../../dataset/nlp/data_cn/test.cn.enc\" )\n",
    "file_ini( \"../../dataset/nlp/data_cn/test.cn.dec\" )\n",
    "\n",
    "# training data\n",
    "for i in range(num_train):\n",
    "    text = f1.readline()\n",
    "    if not file_encoder_decoder_pair( \"../../dataset/nlp/data_cn/train.cn.enc\",\\\n",
    "                                     '../../dataset/nlp/data_cn/train.cn.dec', text, 1):\n",
    "        print i, text\n",
    "\n",
    "# testing data\n",
    "cnt = i\n",
    "text = f1.readline()\n",
    "while text:\n",
    "    cnt+=1\n",
    "    if not file_encoder_decoder_pair( '../../dataset/nlp/data_cn/test.cn.enc',\\\n",
    "                                      '../../dataset/nlp/data_cn/test.cn.dec', text, 1):\n",
    "        print cnt, text\n",
    "    text = f1.readline()\n",
    "\n",
    "print 'DONE!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'dec_vocab_size': 20000,\n",
       " 'enc_vocab_size': 20000,\n",
       " 'layer_size': 256,\n",
       " 'learning_rate': 0.5,\n",
       " 'learning_rate_decay_factor': 0.99,\n",
       " 'max_gradient_norm': 5.0,\n",
       " 'max_train_data_size': 0,\n",
       " 'mode': 'local_test',\n",
       " 'num_layers': 3,\n",
       " 'steps_per_checkpoint': 500,\n",
       " 'test_dec': '../../dataset/nlp/data_cn/test.cn.dec',\n",
       " 'test_enc': '../../dataset/nlp/data_cn/test.cn.enc',\n",
       " 'train_dec': '../../dataset/nlp/data_cn/train.cn.dec',\n",
       " 'train_enc': '../../dataset/nlp/data_cn/train.cn.enc',\n",
       " 'working_directory': 'working_dir_cn/'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    from ConfigParser import SafeConfigParser\n",
    "except:\n",
    "    from configparser import SafeConfigParser\n",
    "    \n",
    "def get_config(config_file='seq2seq.ini'):\n",
    "    parser = SafeConfigParser()\n",
    "    parser.read(config_file)\n",
    "    # get the ints, floats and strings\n",
    "    _conf_ints = [ (key, int(value)) for key,value in parser.items('ints') ]\n",
    "    _conf_floats = [ (key, float(value)) for key,value in parser.items('floats') ]\n",
    "    _conf_strings = [ (key, str(value)) for key,value in parser.items('strings') ]\n",
    "    return dict(_conf_ints + _conf_floats + _conf_strings)\n",
    "\n",
    "gConfig = get_config()\n",
    "\n",
    "gConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_train, dec_train, enc_dev, dec_dev, _, _ = \\\n",
    "data_utils.prepare_custom_data(gConfig['working_directory'],gConfig['train_enc'],gConfig['train_dec'],\\\n",
    "                    gConfig['test_enc'],gConfig['test_dec'],gConfig['enc_vocab_size'],gConfig['dec_vocab_size'] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# statistics on encoder-decoder pairs\n",
    "\n",
    "file_paths = [ \"../../dataset/nlp/data_cn/train.cn.enc.ids20000\",\\\n",
    "               \"../../dataset/nlp/data_cn/train.cn.dec.ids20000\",\\\n",
    "               \"../../dataset/nlp/data_cn/test.cn.enc.ids20000\",\\\n",
    "               \"../../dataset/nlp/data_cn/test.cn.dec.ids20000\" ]\n",
    "\n",
    "for i in file_paths:\n",
    "    \n",
    "    f1 = codecs.open(i, \"r\")\n",
    "        \n",
    "    line = f1.readline()\n",
    "    line_len = []\n",
    "    \n",
    "    while line:\n",
    "        line_len.append( len(line.split(' ')) )\n",
    "        line = f1.readline()\n",
    "        \n",
    "    print i,\"  \", \\\n",
    "          \"Mean:\", sum(line_len)*1.0/len(line_len),\\\n",
    "          \"Max:\", max(line_len),\\\n",
    "          \"Min:\", min(line_len)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test on snownlp and jieba\n",
    "from snownlp import *\n",
    "\n",
    "sentence = u\"   从过去获取记忆的灵感，创造121明天未知12123的潮流，212。@范冰冰@@范冰冰工作室@ 【范冰冰 】@路易威登@： ...\"\n",
    "\n",
    "sentence = u\"海报来也！大家一起数星星→       梁家辉 祖峰    众神归位！请用一个词点评本片阵容，\\\n",
    "评论里最有才的那位，片方将免费送你去上影节“全明星封神之夜”哟！\"\n",
    "\n",
    "sentence = u'???? ????<br/>奥汀子集齐，月刊召唤成功！ 、 、 、 、 、 、 及 携手呈现双封面！明天起将每天揭晓位主创的网络专属单人封面，\\\n",
    "更有可能获得独属你家偶像的签名海报套装，敬请锁定陆续开启的购买链接 ...'\n",
    "\n",
    "sentence = u'期待值up up but fdafa！ 大哥英雄力MAX 星约翰尼•诺克斯维尔,在澳大利亚总理夫人Lucy Turnbull女士的见证下\\\n",
    "先转藏再看by网络 http:t.cn/Rq0Erd'\n",
    "\n",
    "sentence = u'冰冰身着马海毛121运动双排扣大衣132323'\n",
    "\n",
    "sentence = u'#分手大师#好演员'\n",
    "# snownlp\n",
    "s = SnowNLP(sentence)\n",
    "_DIGIT_RE = re.compile(br\"\\d\")\n",
    "\n",
    "# split on words\n",
    "for i in s.words:\n",
    "    \n",
    "    word = re.sub(_DIGIT_RE, b\"0\", i)\n",
    "    if word == i:\n",
    "        print word.encode('utf-8')\n",
    "    else:\n",
    "        print word.encode('utf-8')\n",
    "        \n",
    "        \n",
    "# # jieba\n",
    "# seg_list = jieba.cut(sentence)\n",
    "\n",
    "# tokens = []\n",
    "# for i in seg_list:\n",
    "#     print i\n",
    "#     tokens.append(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
